{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export http_proxy=\"http://127.0.0.1:7890\"\n",
    "!export https_proxy=\"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os \n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "proxy = {\n",
    "    \"http\": \"http://127.0.0.1:7890\",\n",
    "    \"https\": \"http://127.0.0.1:7890\"\n",
    "}\n",
    "\n",
    "requests.get(\"https://huggingface.co/datasets/fka/awesome-chatgpt-prompts\", timeout = 5, proxies = proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import configure_http_backend\n",
    "def backend_factory() -> requests.Session:\n",
    "    session = requests.Session()\n",
    "    session.proxies = proxy\n",
    "    session.verify = False\n",
    "    return session\n",
    "\n",
    "configure_http_backend(backend_factory=backend_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")   \n",
    "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n",
    "\n",
    "# dataset = load_dataset(\"mmlu\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\soft\\Anaconda\\envs\\badouai\\lib\\site-packages\\datasets\\load.py:2555: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=no_checks' instead.\n",
      "  warnings.warn(\n",
      "d:\\soft\\Anaconda\\envs\\badouai\\lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for hendrycks_test contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hendrycks_test\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 107, in <module>\n",
      "    run_experiment(*get_config_and_api_key_name_from_args())\n",
      "  File \"main.py\", line 68, in run_experiment\n",
      "    qwes = get_questions_with_exemplars(\n",
      "  File \"d:\\ai-code\\leveraging-llms-for-mcqa\\dataset_utils.py\", line 57, in get_questions_with_exemplars\n",
      "    return info(n_shots=n_shots, do_strong_shuffle=do_strong_shuffle)\n",
      "  File \"d:\\ai-code\\leveraging-llms-for-mcqa\\dataset_utils.py\", line 181, in load_mmlu\n",
      "    name_qwes = get_questions_with_exemplars(\n",
      "  File \"d:\\ai-code\\leveraging-llms-for-mcqa\\dataset_utils.py\", line 60, in get_questions_with_exemplars\n",
      "    exemplar_ds = load_fn(\n",
      "  File \"d:\\ai-code\\leveraging-llms-for-mcqa\\dataset_utils.py\", line 37, in load_hf_dataset_no_verify\n",
      "    return load_dataset(\n",
      "  File \"d:\\soft\\Anaconda\\envs\\badouai\\lib\\site-packages\\datasets\\load.py\", line 2609, in load_dataset\n",
      "    builder_instance.download_and_prepare(\n",
      "  File \"d:\\soft\\Anaconda\\envs\\badouai\\lib\\site-packages\\datasets\\builder.py\", line 1027, in download_and_prepare\n",
      "    self._download_and_prepare(\n",
      "  File \"d:\\soft\\Anaconda\\envs\\badouai\\lib\\site-packages\\datasets\\builder.py\", line 1789, in _download_and_prepare\n",
      "    super()._download_and_prepare(\n",
      "  File \"d:\\soft\\Anaconda\\envs\\badouai\\lib\\site-packages\\datasets\\builder.py\", line 1122, in _download_and_prepare\n",
      "    self._prepare_split(split_generator, **prepare_split_kwargs)\n",
      "  File \"d:\\soft\\Anaconda\\envs\\badouai\\lib\\site-packages\\datasets\\builder.py\", line 1587, in _prepare_split\n",
      "    split_info = self.info.splits[split_generator.name]\n",
      "  File \"d:\\soft\\Anaconda\\envs\\badouai\\lib\\site-packages\\datasets\\splits.py\", line 532, in __getitem__\n",
      "    instructions = make_file_instructions(\n",
      "  File \"d:\\soft\\Anaconda\\envs\\badouai\\lib\\site-packages\\datasets\\arrow_reader.py\", line 116, in make_file_instructions\n",
      "    raise TypeError(f\"Expected str 'name', but got: {type(name).__name__}\")\n",
      "TypeError: Expected str 'name', but got: NoneType\n"
     ]
    }
   ],
   "source": [
    "!python main.py mmlu codex natural 1 openai --do_strong_shuffle --do_perm\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "badouai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
